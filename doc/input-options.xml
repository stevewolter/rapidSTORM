<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE chapter
[<!ENTITY % isogrk1 PUBLIC "ISO 8879:1986//ENTITIES Greek Letters//EN//XML"
                    "http://www.oasis-open.org/docbook/xmlcharent/0.3/iso-grk1.ent">
 <!ENTITY % isopub PUBLIC "ISO 8879:1986//ENTITIES Publishing//EN//XML"
                    "http://www.oasis-open.org/docbook/xmlcharent/0.3/iso-pub.ent">
%isogrk1;
%isopub;
]>

<chapter id="input_options" xmlns='http://docbook.org/ns/docbook' xml:lang="en" xmlns:et="http://super-resolution.de/elemtable">
   <et:title desc="Input options" topic="Input"/>
<glossary>
   <title/>
   <glossdiv>
      <title>Input fields</title>
      <et:elem desc="Number of input channels" topic="ChannelCount">
            <para>This field is used to declare how many <xref 
                  linkend="input_channels">input channels</xref> should be
            made available. New input channel configuration tabs are added
            or removed as needed when this field is changed. </para>
      </et:elem>
      <et:elem desc="Input type" topic="InputType">
            <para>This field is used to select the general type of input that should
            be used. If input should be read from some kind of file, select
            <et:elemref linkend="FileMethod"/> here and the specific type of file in the 
            <et:elemref linkend="FileType"/> field.</para>
            <glossseealso otherterm="FileMethod"/>
      </et:elem>
      <et:elem desc="Input file" topic="InputFile">
            <para>The location of the input data is selected in this field. You can put
            any existing file here. It's interpretation (file type) and file-type-specific
            input options are set in the <et:elemref linkend="FileType"/> field.
            This field is only available when you have selected <et:elemref linkend="FileMethod"/>
            as <et:elemref linkend="InputType"/>.</para>
            <glossseealso otherterm="FileMethod"/>
            <glossseealso otherterm="FileType"/>
      </et:elem>
      <et:elem desc="File type" topic="FileType">
            <para>This field is used to select the type of data that are present in the
            input file. Usually, this is auto-detected, and when the 
            <et:elemref linkend="InputFile"/> location is changed, this
            field is updated with the auto-determined file type for the selected
            input file.</para>
            <glossseealso otherterm="FileMethod"/>
            <glossseealso otherterm="InputFile"/>
      </et:elem>
      <et:elem desc="Input method" topic="InputMethod">
            <para>This field is used to select the type of data that are present in the
            input file. Usually, this is auto-detected, and when the 
            <et:elemref linkend="InputFile"/> location is changed, this
            field is updated with the auto-determined file type for the selected
            input file.</para>
            <para>You can enforce the file type instead of auto-detection by explicitly selecting
                a choice in this field after entering a file name.</para>
      </et:elem>

      <et:elem desc="First image" topic="FirstImage">
            <para>This option allows skipping of the initial frames of the input. The
               frames are indexed sequentially from 0, and the value given here is 
               inclusive. Consequently, the default value of 0 in this field causes
               all images to be loaded. A value of 5 would skip the first 5 images.</para>
            <glossseealso otherterm="LastImage"/>
      </et:elem>

      <et:elem desc="Last image" topic="LastImage">
        <para>If this option is enabled, it allows limiting the length of the computation.
            All images past the given frame number, counted from 0 and excluding the given
            number, are skipped. For example, a value of 3 would case the four images with
            indices 0, 1, 2 and 3 to be computed.</para>
        <glossseealso otherterm="FirstImage"/>
      </et:elem>

      <et:elem desc="Mirror input data along Y axis" topic="MirrorY">
        <para>If this option is enabled, each input image is flipped such that the topmost row
            in each image becomes the bottommost row.</para>
      </et:elem>

      <et:elem desc="Dual view" topic="DualView">
        <para>This option allows splitting an image that contains two logical layers. For example,
            if the reflected beam of a beam splitter is projected onto the left side of the
            camera and the transmitted on the right side, this output allows to separate them into
            two different layers that can be used for 3D estimation.</para>
        <para>The options are <guilabel>None</guilabel>, which indicates no splitting at all,
            <guilabel>Left and right</guilabel>, which splits the image at X = Width/2, and
            <guilabel>Top and bottom</guilabel>, which splits the image at Y = Height/2.</para>
        <para>The dual view output always splits images exactly in the middle. Use plane
            alignment to configure other shifts.</para>
      </et:elem>

      <et:elem topic="OnlyPlane" desc="Processed planes">
        <para>This option allows selecting a single layer from a multi-layer acquisition.
            The default option, <guilabel>All planes</guilabel>, has no effect. All other
            options select a single layer from the available input layers, which will be
            the only layer that is processed.</para>
      </et:elem>

      <et:elem topic="CountsPerPhoton" desc="Camera response to photon">
        <para>This option allows to give the mean intensity each photon generates on the camera.
            In other terms, a value of 5 in this field assumes that the value of a pixel goes up
            by 5 (on average) every time the pixel is hit by a photon.</para>
      </et:elem>

      <et:elem topic="DarkCurrent" desc="Dark intensity">
            <para>This option allows to give the dark intensity of the camera, i.e. the mean pixel value
               for a pixel that is hit by no photons during the integration time.</para>
      </et:elem>

      <et:elem topic="Alignment" desc="Plane alignment">
            <para>This option allows to describe the mapping from input pixels to sample space coordinates.
               For the majority of single-layer applications, the default choice of <et:elemref linkend="ScaledProjection"/>
               is appropriate.</para>
            <glossseealso otherterm="ScaledProjection"/>
            <glossseealso otherterm="AffineProjection"/>
            <glossseealso otherterm="SupportPointProjection"/>
      </et:elem>

      <et:elem topic="ZCalibration" desc="Z calibration file">
        <para>The filename of a Z calibration file (<xref linkend="z_calibration_table"/>)
            should be given in this option. The calibration file will be used to determine 
            PSF widths as a function of the Z coordinate during fitting.</para>
      </et:elem>

      <et:elem desc="Join inputs on" topic="JoinOn">
            <para>When multiple channels are selected, this field is used to declare how these
            multiple channels are combined into a single image or dataset. The available options are:</para>
            <orderedlist>
            <listitem>Spatial joining in X/Y dimension, which means that images or datasets are pasted
            next to each other. For spatial joining of image data in the X/Y dimension, the not-joined dimension
            (e.g. the Y dimension for X joining) must match.</listitem>
            <listitem>Spatial joining in Z dimension. When processing images, each input channel is 
               considered as a separate layer, with the first channel forming the first layer. For
               localization data, this is identical to X/Y joining in Z dimension.</listitem>
            <listitem>Joining in time. Channels are processed after each other in the sequence of their
               declaration. For images, the dimensions of the images must match.</listitem>
            </orderedlist>
      </et:elem>
      <et:elem desc="Output file basename" topic="Basename">
            <para>This is the location and filename prefix for rapidSTORM output files. More precisely,
                the filenames for rapidSTORM's output files are produced by adding a file-specific suffix
                to the value of this field.</para>
            <para>This field is automatically set when a new input file is selected.</para>
      </et:elem>
      <et:elem desc="Fluorophore types" topic="FluorophoreCount">
            <para>The number of <xref linkend="fluorophore"/>s present in the input should be given here.
            When multiple fluorophores are selected, <et:elemref linkend="TransmissionCoefficient"/>
            fields can be used to characterize the spectra.</para>
      </et:elem>
      <et:elem desc="Size of one input pixel" topic="PixelSizeInNM">
         <para>This field gives the size of the sample part that is imaged in a single camera pixel.
         Typically, this value should be in the order of 100 nm. See <xref linkend="Thompson2002"/>
         for a discussion about ideal values.</para>
      </et:elem>
      <et:elem desc="PSF FWHM" topic="PSF">
            <para>The full width at half maximum of the optical point spread function. More
            precisely, the typical width of an emitter's image should be entered here, including fluorophore
            size and camera pixelation effects. rapidSTORM will fit spots in the images with a Gaussian 
            with the same FWHM as given here. If the PSF is unknown, it can be determined semi-automatically
            by using the <xref linkend="estimate_psf_form"/> output.</para>
      </et:elem>

    <et:elem topic="ThreeD" desc="3D PSF model">
        <para>
            The 3D PSF model denotes the functional form of the PSF's change with respect to the 
            emitter's Z position.
        </para>
        <glossseealso otherterm="No3D"/>
        <glossseealso otherterm="Polynomial3D"/>
        <glossseealso otherterm="Spline3D"/>
    </et:elem>
    <et:elem topic="No3D" desc="No 3D">
        <para>The PSF width is constant, and no Z coordinate is considered.</para>
    </et:elem>
    <et:elem topic="Polynomial3D" desc="Polynomial 3D">
        <para>The polynomial 3D model (see <xref linkend="polynomial_3d"/>) determines PSF widths from the Z coordinate.</para>
    </et:elem>
    <et:elem topic="Spline3D" desc="Interpolated 3D">
        <para>The piecewise cubic 3D model (see <xref linkend="interpolated_3d"/>) determines
        PSF widths from the Z coordinate.</para>
    </et:elem>

    <et:elem topic="SharpestPSF" desc="PSF FWHM at sharpest Z">
        <para>The width of the PSF at the Z position indicated by <et:elemref linkend="ZPosition"/>. This is typically
        the same value as <et:elemref linkend="PSF"/>.</para>
    </et:elem>
    <et:elem topic="ZPosition" desc="Point of sharpest Z">
        <para>The Z coordinate of the focal plane (the plane where the PSF has the lowest width).
        You can indicate astigmatism by giving different positions for the two dimensions
        of one layer, or biplane by giving different positions for two layers.</para>
    </et:elem>
    <et:elem topic="ZRange" desc="Maximum Z range">
        <para>The PSF model will be considered valid up to this distance from the point of sharpest Z.
        Any considered Z coordinate further away than this value will be immediately discarded.</para>
    </et:elem>
    <et:elem topic="WideningConstants" desc="Widening slopes">
        <para>These entries give the speed of PSF growth. They have to be determined experimentally, and 
        we know of no reliable method to do so. For more information, see <xref linkend="polynomial_3d"/>.</para>
    </et:elem>

    <et:elem desc="Localizations file" topic="STM">
      <para>This input driver can be used to read the files written
         by the <xref linkend="localizations_file_output"/> output module.
         The file format is documented with the output module.</para>
    </et:elem>

      <et:elem topic="AndorSIF" desc="Andor SIF file">
        <para>This input driver can read the Andor SIF file format produced
            by Andor Technology software such as SOLIS.</para>
        <para>SIF files are stored in an uncompressed binary format with a
            simple text header. Because reading SIF files cannot be implemented
            in a forward-compatible way (reading new SIF files with old software),
            this driver might be unable to open the file; in this case, an error
            message is shown indicating the maximum known and the encountered
            version of the Andor SIF structure. Please obtain a newer version
            of this software in this case.</para>
      </et:elem>

      <et:elem topic="TIFF" desc="TIFF file">
        <para>This input driver reads a multipage TIFF stack. All images in the
            TIFF stack must have the same size, and be greyscale images of up to
            64 bit depth. Both integer and floating point data are allowed, even
            though all data are converted internally to 16 bit unsigned format.</para>
      </et:elem>

    <et:elem topic="ScaledProjection" desc="No alignment">
        <para>The upper left pixel is assumed to be at (0,0) nanometers. The 
        <et:elemref linkend="PixelSizeInNM"/> field gives the offset of each pixel to the next. For example, if
        the pixel sizes are 100 nm in X and 110 nm in Y, the pixel at (10,15) is at (1,1.65) &mgr;m.
        </para>
    </et:elem>
    <et:elem topic="AffineProjection" desc="Linear alignment">
         <para>Naive pixel coordinates are computed identically to <et:elemref linkend="ScaledProjection"/>
            and then transformed by an affine transformation matrix.</para>
    </et:elem>
    <et:elem topic="SupportPointProjection" desc="Support point alignment">
         <para>Naive pixel coordinates are computed identically to <et:elemref linkend="ScaledProjection"/>.
            A nonlinear transformation image is read from a file. The naive coordinates are projected
            into the source image of the nonlinear transformation, and the transformation is applied with linear
            interpolation.</para>
    </et:elem>
    <et:elem topic="SupportPointAlignmentFile" desc="bUnwarpJ transformation">
        <para>A bUnwarpJ <emphasis>raw</emphasis> transformation file is given in this field to characterize
            the channel alignment.</para>
    </et:elem>
    <et:elem topic="SupportPointResolution" desc="Transformation resolution">
        <para>The resolution (pixel size) of the file given in <et:elemref linkend="SupportPointAlignmentFile"/>.
            This resolution is not necessarily the same as the value of <et:elemref linkend="PixelSizeInNM"/>.
            For example, you can super-resolve two images with an easily aligned structure such as the nuclear pore
            complex (<xref linkend="Loeschberger2012"/>), which will result in a raw transformation with a 10 nm
            resolution.</para>
    </et:elem>
    <et:elem topic="AlignmentFile" desc="Plane alignment file">
        <para>A plain text file containing a 3x3 affine matrix for linear alignment, with the translation
            given in meters. The matrix
            is assumed to yield the aligned coordinate positions if multiplied with a vector (x, y, 1),
            where x and y are the unaligned coordinates. The upper left
            2x2 part of the matrix is a classic rotation/scaling matrix, the elements (0,2) and (1,2)
            give a translation. For now, the bottom row must be (0,0,1), i.e. not a projective transform.</para>
    </et:elem>

    <et:elem topic="FileMethod" desc="File">
        <para>This choice for <et:elemref linkend="InputMethod"/> indicates that a file is used for input instead
            of generating the input stochastically or reading directly from a camera.</para>
    </et:elem>

    <et:elem topic="TransmissionCoefficient" desc="Transmission of fluorophore N">
        <para>There is one transmission coefficient field for each layer and fluorophore.
            The fields give the relative intensities of each fluorophore in each layer,
            and will be used as scaling factors for the intensity for multi-colour inference
            or for biplane imaging.</para>
        <para>As an example, values of 0.1 in layer 1, fluorophore 0 and 0.9 in layer 2, fluorophore 0 
            would indicate that 90\% of the photons arrive on the second camera and 10\% on the
            first.</para>
    </et:elem>
</glossdiv>
</glossary>
</chapter>
